Chapter: Clustering and classification
---

**About the Data set**

Boston data set has 506 observations and 14 variables. It is one of the pre installed data sets that comes with the Mass Package. It contains information regarding the Housing values in suburbs of Boston.

The variables involved are as follows:

crim - per capita crime rate by town.

zn - proportion of residential land zoned for lots over 25,000 sq.ft.

indus - proportion of non-retail business acres per town.

chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

nox - nitrogen oxides concentration (parts per 10 million).

rm - average number of rooms per dwelling.

age - proportion of owner-occupied units built prior to 1940.

dis - weighted mean of distances to five Boston employment centres.

rad - index of accessibility to radial highways.

tax - full-value property-tax rate per $10,000.

ptratio - pupil-teacher ratio by town.

black - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

lstat - lower status of the population (percent).

medv - median value of owner-occupied homes in $1000s.

Source
*Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.*
*Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.*


##Loading the Data Set, Installing package and Exploring Data set

```{r}
#Loading required libraries for the Exercise

library(ggplot2)
library(dplyr)
library(corrplot)
library(GGally)
library(tidyr)
library(MASS)

#Loading Data Set
data("Boston")

#Exploring the Data Set
str(Boston)
dim(Boston)
summary(Boston)
```

**Observation:**
*The data set has 506 Observations of 14 variables*
*The variables chas and rad are integers whiles others are numerical.*
*From the function dimensions, from the Min. and Max range we can see that the variable distribution is skewed except for the variables rm, and medv.*

##Graphical Distribution and summary of variables

```{r}
# plot matrix of the variables
pairs(Boston)

# printing the correlation matrix to look at the correlations between variables in the data
cor_matrix <- cor(Boston) %>% round(digits = 2)

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6,)
```

**Observations**

**Pair Plot:** *The pair plot is the visual representation of the observation as mentioned before this*

**Correlation Plot:** *According to the Correlation plot, we can say that there many variables with high positive correlation for example the variables 'rad' and 'tax' have the highest positive correlation among all the variables. On the other hand, we can also see strong negative correlation between many variable, highest negative correlation being among the variable like 'lstat' and 'medv', 'age' and 'dis', 'nox' and 'dis', and, 'indus' and 'dis'. Therefore, we can also say that 'dis' (distance) has the most highest negative correlation among all the other variables*

##Standardization of Data Set and Categorization of Variable

1. Standardization of data Set
*It is the process through which we take the mean of the relevant columns, subtract them, then divide the result by the standard deviation.*

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(scale(Boston))

summary(boston_scaled)
```

**Observation**
*We can see that now all variables are normally distributed and because of the standardization process, it is done in such a way that mean of all variables is equal to zero*

2.Creating a categorical variable of the crime rate in the Boston data set

```{r}
# Create a quantile vector of crim and create the categorical "crime".
bins <- quantile(boston_scaled$crim)

crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c('low','med_low','med_high','high'))

# Remove the original unscaled variable and add the new categorical value to scaled data.
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)

# Look at the table of the new factor crime
table(boston_scaled$crim)
```

**Observation**
*We have now created a categorical variable(previously continuous) and changed the name from crim to crime which has now been added as a separate variable in the boston_scaled data set which has 127 observation for low crime, 126 for medium to low and medium to high crime and 127 for high crime*

##Divide the dataset to train and test sets.

*When we want to use a statistical method to predict something, it is important to have data to test how well the predictions fit. Splitting the original data to test and train sets allows us to check how well our model works.  The training of the model is done with the train set and prediction on new data is done with the test set. This way you have true classes / labels for the test data, and you can calculate how well the model performed in prediction.*

*Below the size = n x 0.8 representation means that it will randomly choose the 80% of the data to create a training data set*

```{r}

# Number of rows in the Boston dataset
n <- nrow(boston_scaled) 

# Choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# Create train set
train <- boston_scaled[ind,]

# Create test set
test <- boston_scaled[-ind,] 

# Save the correct classes from the test data
correct_classes <- test$crime

# Remove the crime variable from the test data
test <- dplyr::select(test, -crime)

```


##Linear Discriminant Analysis on the train set

*LDA is done to find the linear combination among the variable which separates the classes of target variables* *For this analysis, we will use  'Crime' as the target variable*

```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(factor(train$crime))

# plot the lda results
plot(lda.fit, col = classes, pch = classes, dimen = 2)
lda.arrows(lda.fit, myscale = 1)
```

**Observation**
*The proportion of trace tells us that LD1 explains variance up-to 96.27%, LD2 does it up to 2.84% and LD3 does it for below 1% i.e., at around 0.89%.*
*Through the plot we can say that the target variable crime is clearly separated, and the variable accessibility to 'rad' is optimal as seen through the arrow.*

##Prediction and Cross tabulation

**(Please Note:***The saving of crime categories and removal of the categorical crime variable from the test data set can be found just before the start of LDA exercise in the Divide the data set to train and test sets exercise)*

```{r}
# Predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# Cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

**Observation**
*The cross tabulation suggests that the model predicts the 'high' crime rate quite well. There are 102 total observation and The majority of the predicted values are predicted as correct values for the same category making the model usable for rough predictions.* 

##Reload and standardize the Boston dataset 

```{r}
# Reload Boston dataset.
data("Boston")

# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame from matrix type.
boston_scaled <- as.data.frame(boston_scaled)
```

##Calculate the distance between Observations, K means clustering and visualisation

```{r}
# Calculate the Euclidean distances between observations.
dist_eu <- dist(boston_scaled, method = "euclidean")

# look at the summary of the distances
summary(dist_eu)
```

**Observation**
*The most normal distance measure is Euclidean, according to which the distance range is between 0.1343 to 14.3970, with a mean of 4.9111 and median of 4.8241*

```{r}
# K-means clustering
km <-kmeans(boston_scaled, centers = "3")

# plot the Boston dataset with clusters
pairs(boston_scaled[6:10], col = km$cluster)
```

**Observation**
*As I have clustered the data into 5 columns and definde k-means as 3 cluster*

*We will now find the optimal number for clusters*

```{r}
# Investigate optimal number of clusters and rerun algorithm
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# Visualize the cluster.
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

**Observation**
*We can see that optimal number drops radically and with this we can see that after every two cluster, there seems to be a change.*

*Let us now check how does it look with optimal number 2*

```{r}
# k-means clustering
km <-kmeans(dist_eu, centers = 2) 

# Plot the clusters
pairs(boston_scaled[6:10], col = km$cluster)
```

**Observation**
*We can see that because of our previous observation was of 3 clusters and optimal is 2, there isnt much difference but going by the results, 'two' seems to be the optimal number for clustering the data, when the method chosen is Euclidean*

##Super Bonus: Create a 3-D plot

```{r}
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

library(plotly)

# Plot.
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)

# k-means with 4 clusters to compare the methods.
km3D <-kmeans(boston_scaled, centers = 4)


#Plot with km cluster as color of data
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km3D$cluster[ind])
```

**Observation**

*Through the above super cool 3D plots, my observation is that the k-means clustering works better. In the crime plot, it was hard to establish which variables are falling where as some of them got a bit submerged in each other.*
